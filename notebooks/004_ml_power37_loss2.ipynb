{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skeleton pytorch with PyPSA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "# from torchvision.utils import make_grid\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12,4)\n",
    "# matplotlib.rcParams['figure.facecolor'] = '#ffffff'\n",
    "\n",
    "# Logging ML\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd785cd9c30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict(    \n",
    "    learning_rate =1e-3,\n",
    "    # batch_size =128,\n",
    "    epochs = int(3e3),\n",
    "    model=\"nn3\",\n",
    "    layers = [256,128,128,64],\n",
    "    training_loss = \"MSE+tP\",\n",
    "    alpha= 1e-6,\n",
    "    # scheduler = \"one-cycle-lr\",\n",
    "    years=[\"2011\",\"2013\",\"2014\"],\n",
    "    years_val=[\"2012\"],\n",
    "    nodes=\"37\"\n",
    ")\n",
    "use_tb = False\n",
    "use_wandb= True\n",
    "manual_logging = False\n",
    "check_data_with_plots = False\n",
    "\n",
    "project_name = f\"phd-ph5x-02-power_prediction_{config['nodes']}\"\n",
    "\n",
    "random_seed = 746435\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otherwise all files present\n"
     ]
    }
   ],
   "source": [
    "dir_root = Path(\"../\") # go to root of git repo\n",
    "dir_data = dir_root / \"data\"\n",
    "dir_data_ml= dir_data /\"ml\"\n",
    "dir_models = dir_root / \"models\"\n",
    "dir_runs = dir_root/\"runs\"\n",
    "dir_runs_tb = dir_runs /\"tb\"\n",
    "dir_runs_wandb = dir_root / \"wandb\"\n",
    "param_save = \"002_01_simplest\"\n",
    "\n",
    "network_name = f\"elec_s_{config['nodes']}_ec_lcopt_Co2L-3H\"\n",
    "\n",
    "dir_training_set = [dir_data_ml / y / \"3M\" for y in config[\"years\"]]\n",
    "filenames_inputs_tr = [d / f\"{network_name}_inputs.P\" for d in dir_training_set]\n",
    "filenames_outputs_tr = [d / f\"{network_name}_outputs_p.P\" for d in dir_training_set]\n",
    "\n",
    "dir_val_set = [ dir_data_ml / y/ \"3M\" for y in  config[\"years_val\"]]\n",
    "filenames_inputs_val = [d / f\"{network_name}_inputs.P\" for d in dir_val_set]\n",
    "filenames_outputs_val = [d / f\"{network_name}_outputs_p.P\" for d in dir_val_set]\n",
    "\n",
    "for fn in [*filenames_inputs_tr, *filenames_outputs_tr,\n",
    "           *filenames_inputs_val,*filenames_outputs_val]:\n",
    "    if not fn.exists():\n",
    "        print(f\"{fn}: Missing\")\n",
    "print(\"Otherwise all files present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 37, 8760, 2928)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_all_dfs(filenames):\n",
    "    return pd.concat([pd.read_pickle(f) for f in filenames])\n",
    "\n",
    "df_input_tr = read_all_dfs(filenames_inputs_tr)\n",
    "df_output_tr = read_all_dfs(filenames_outputs_tr)\n",
    "df_input_val = read_all_dfs(filenames_inputs_val)\n",
    "df_output_val = read_all_dfs(filenames_outputs_val)\n",
    "\n",
    "\n",
    "assert (df_input_val.columns==df_input_tr.columns).all(), \"Mismatch in input columns\"\n",
    "assert (df_output_val.columns==df_output_tr.columns).all(), \"Mismatch in output columns\"\n",
    "input_features = df_input_val.columns\n",
    "output_features = df_output_val.columns\n",
    "\n",
    "x_train = torch.from_numpy(df_input_tr.values.astype(\"float32\"))\n",
    "y_train = torch.from_numpy(df_output_tr.values.astype(\"float32\"))\n",
    "x_val = torch.from_numpy(df_input_val.values.astype(\"float32\"))\n",
    "y_val = torch.from_numpy(df_output_val.values.astype(\"float32\"))\n",
    "\n",
    "n_input = x_train.shape[1]\n",
    "n_output = y_train.shape[1]\n",
    "n_samples_tr = x_train.shape[0]\n",
    "n_samples_val = x_val.shape[0]\n",
    "\n",
    "# Normalization defined by training data\n",
    "x_mean = x_train.mean(dim = 0)\n",
    "x_std =x_train.std(dim = 0)\n",
    "y_mean = torch.zeros(n_output)  # centered already\n",
    "y_std = y_train.std(dim = 0)\n",
    "\n",
    "def x_norm(x): return (x-x_mean)/x_std\n",
    "def y_norm(y): return (y-y_mean)/y_std\n",
    "def x_renorm(x): return x*x_std+x_mean\n",
    "def y_renorm(y): return y*y_std+y_mean\n",
    "\n",
    "\n",
    "x_train =  x_norm(x_train)\n",
    "y_train = y_norm(y_train)\n",
    "\n",
    "x_val = x_norm(x_val)\n",
    "y_val = y_norm(y_val)\n",
    "y_renorm(y_train).sum(dim=1)\n",
    "assert not(((x_val[0:100]-x_train[0:100])<1e-5).all()), \"Training data identical to validation data\"\n",
    "\n",
    "\n",
    "(n_input,n_output,n_samples_tr,n_samples_val)\n",
    "# train_loader = load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_data_with_plots:\n",
    "    n_min=min(n_samples_val,n_samples_tr)\n",
    "    _=plt.plot(x_train[:n_min], \"r\", alpha = 0.1) # [:,38:192]\n",
    "    _=plt.plot(y_train[:n_min], \"b\", alpha = 0.1) # [:,38:192]\n",
    "    _=plt.plot(x_val[:n_min], \"m\", alpha = 0.1) # [:,38:192]\n",
    "    _=plt.plot(y_val[:n_min], \"c\", alpha = 0.1) # [:,38:192]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Full model\n",
    "class PowerModel(nn.Module):\n",
    "    \"\"\"Feedfoward neural network (0 layers)\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(PowerModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        current_dim = input_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "        for hdim in hidden_dim:\n",
    "            self.layers.append(nn.Linear(current_dim, hdim))\n",
    "            current_dim = hdim\n",
    "        self.layers.append(nn.Linear(current_dim, output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        out = self.layers[-1](x)\n",
    "        return out    \n",
    "        \n",
    "(\n",
    "    # def training_step(self, batch):\n",
    "    #     inputs, targets = batch \n",
    "    #     out = self(inputs)                  # Generate predictions\n",
    "    #     loss = F.cross_entropy(out, targets) # Calculate loss\n",
    "    #     return loss\n",
    "    \n",
    "    # def validation_step(self, batch):\n",
    "    #     inputs, targets = batch \n",
    "    #     out = self(inputs)                    # Generate predictions\n",
    "    #     loss = F.cross_entropy(out, targets)   # Calculate loss\n",
    "    #     # acc = accuracy(out, labels)           # Calculate accuracy\n",
    "    #     return loss\n",
    "\n",
    "        \n",
    "    # def validation_epoch_end(self, outputs):\n",
    "    #     batch_losses = outputs\n",
    "    #     epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "    #     # batch_accs = [x['val_acc'] for x in outputs]\n",
    "    #     # epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "    #     return  epoch_loss.item()\n",
    "\n",
    "    # def epoch_end(self,epoch,result):\n",
    "    #     print(f\"Epoch [{epoch}], val_loss: { result['val_loss'] :.4f}\") #, val_acc: {result['val_acc']:.4f}\")\n",
    ")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NN regression model\n",
    "if config[\"model\"]==\"linear\":\n",
    "    model = nn.Linear(n_input,n_output)\n",
    "elif config[\"model\"].startswith(\"nn\"):\n",
    "    model =  PowerModel(n_input,n_output,hidden_dim=config[\"layers\"])\n",
    "    \n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()  \n",
    "def mean_total_power(y): return (y_renorm(y).sum(dim=1)**2).mean(dim=0) #squared\n",
    "# criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "if \"scheduler\" in config:\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,config[\"learning_rate\"],config[\"epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33melkir\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../runs/wandb/run-20221008_014038-u0ra1eqx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/elkir/phd-ph5x-02-power_prediction_37/runs/u0ra1eqx\" target=\"_blank\">playful-dream-20</a></strong> to <a href=\"https://wandb.ai/elkir/phd-ph5x-02-power_prediction_37\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiation_MSEloss: 1.9856770038604736\n",
      "Epoch [100], TLoss: 1.9282768, MSELoss: 1.9254757, Val_loss: 2.04153\n",
      "Epoch [200], TLoss: 1.1767964, MSELoss: 1.1746836, Val_loss: 1.32994\n",
      "Epoch [300], TLoss: 1.0447116, MSELoss: 1.0397955, Val_loss: 1.18101\n",
      "Epoch [400], TLoss: 0.9905174, MSELoss: 0.9875436, Val_loss: 1.12921\n",
      "Epoch [500], TLoss: 1.0123465, MSELoss: 0.9485180, Val_loss: 1.09685\n",
      "Epoch [600], TLoss: 0.9078457, MSELoss: 0.9013402, Val_loss: 1.07245\n",
      "Epoch [700], TLoss: 0.9025121, MSELoss: 0.8615807, Val_loss: 1.07214\n",
      "Epoch [800], TLoss: 0.8622912, MSELoss: 0.8096967, Val_loss: 1.05096\n",
      "Epoch [900], TLoss: 0.7945005, MSELoss: 0.7741361, Val_loss: 1.03323\n",
      "Epoch [1000], TLoss: 0.7579427, MSELoss: 0.7470080, Val_loss: 1.02004\n",
      "Epoch [1100], TLoss: 0.7391319, MSELoss: 0.7051849, Val_loss: 0.99044\n",
      "Epoch [1200], TLoss: 0.6628426, MSELoss: 0.6598454, Val_loss: 0.96676\n",
      "Epoch [1300], TLoss: 0.6330899, MSELoss: 0.6227826, Val_loss: 0.95059\n",
      "Epoch [1400], TLoss: 0.5952615, MSELoss: 0.5926965, Val_loss: 0.93533\n",
      "Epoch [1500], TLoss: 0.5738450, MSELoss: 0.5720326, Val_loss: 0.92346\n",
      "Epoch [1600], TLoss: 0.5619845, MSELoss: 0.5472484, Val_loss: 0.90679\n",
      "Epoch [1700], TLoss: 0.5285798, MSELoss: 0.5268987, Val_loss: 0.89484\n",
      "Epoch [1800], TLoss: 0.5133686, MSELoss: 0.5115132, Val_loss: 0.88291\n",
      "Epoch [1900], TLoss: 0.5146825, MSELoss: 0.5059066, Val_loss: 0.87986\n",
      "Epoch [2000], TLoss: 0.5394730, MSELoss: 0.4898755, Val_loss: 0.87189\n",
      "Epoch [2100], TLoss: 0.4951114, MSELoss: 0.4712941, Val_loss: 0.86304\n",
      "Epoch [2200], TLoss: 0.4671206, MSELoss: 0.4561231, Val_loss: 0.85242\n",
      "Epoch [2300], TLoss: 0.4510143, MSELoss: 0.4465367, Val_loss: 0.85153\n",
      "Epoch [2400], TLoss: 0.4358706, MSELoss: 0.4340158, Val_loss: 0.84565\n",
      "Epoch [2500], TLoss: 0.4331519, MSELoss: 0.4240201, Val_loss: 0.84440\n",
      "Epoch [2600], TLoss: 0.4222696, MSELoss: 0.4188370, Val_loss: 0.84167\n",
      "Epoch [2700], TLoss: 0.4069411, MSELoss: 0.4056683, Val_loss: 0.84057\n",
      "Epoch [2800], TLoss: 0.4391719, MSELoss: 0.3993836, Val_loss: 0.84530\n",
      "Epoch [2900], TLoss: 0.4009041, MSELoss: 0.3899060, Val_loss: 0.84122\n",
      "Epoch [3000], TLoss: 0.3961221, MSELoss: 0.3813110, Val_loss: 0.84152\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc6d4523d4b4e55af6cbf910f61d57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>diff_loss</td><td>▁▁▂▂▂▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇████</td></tr><tr><td>loss</td><td>██▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_mtP</td><td>▄▁▂▁▁▁▁▃▁▃▂▁▁▃▃▁▁▂▁▂▁▂▁▂▃▁▂▂▃▂▁▃▆▂▃▁█▁█▁</td></tr><tr><td>loss_val</td><td>██▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_val_mtP</td><td>▄▁▂▁▁▁▁▃▁▃▂▁▁▃▃▁▁▂▁▂▁▂▁▂▄▁▂▂▃▂▁▃▆▂▃▁█▁█▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total_loss</td><td>██▅▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>diff_loss</td><td>0.46021</td></tr><tr><td>loss</td><td>0.38131</td></tr><tr><td>loss_mtP</td><td>0.01481</td></tr><tr><td>loss_val</td><td>0.84152</td></tr><tr><td>loss_val_mtP</td><td>0.01398</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>total_loss</td><td>0.39612</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">playful-dream-20</strong>: <a href=\"https://wandb.ai/elkir/phd-ph5x-02-power_prediction_37/runs/u0ra1eqx\" target=\"_blank\">https://wandb.ai/elkir/phd-ph5x-02-power_prediction_37/runs/u0ra1eqx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../runs/wandb/run-20221008_014038-u0ra1eqx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "if manual_logging: losses=[]\n",
    "if use_tb: tb_writer= SummaryWriter(log_dir=dir_runs_tb)\n",
    "if use_wandb: wandb.init(\n",
    "    project=project_name,\n",
    "    dir=dir_runs,config=config)\n",
    "\n",
    "print(f\"Instantiation_MSEloss: {criterion(y_train,model(x_train))}\")\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss_mtP = config['alpha']*mean_total_power(outputs)\n",
    "    \n",
    "    outputs_val = model(x_val)\n",
    "    loss_val = criterion(outputs_val, y_val)\n",
    "    loss_val_mtP = config['alpha']*mean_total_power(outputs_val)\n",
    "    diff_loss= loss_val-loss\n",
    "    \n",
    "    total_loss = loss + loss_mtP\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    # model.epoch_end(epoch,loss)\n",
    "    if \"scheduler\" in config: scheduler.step()\n",
    "    \n",
    "    # Logging\n",
    "    if manual_logging: losses.append(loss.item())\n",
    "    if use_wandb: wandb.log(dict(\n",
    "        total_loss=total_loss,\n",
    "        loss=loss,\n",
    "        loss_mtP=loss_mtP,\n",
    "        loss_val=loss_val,\n",
    "        loss_val_mtP=loss_val_mtP,\n",
    "        diff_loss=diff_loss,\n",
    "        \n",
    "        lr=optimizer.param_groups[0]['lr']\n",
    "        ))\n",
    "    if use_tb:\n",
    "        tb_writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "        tb_writer.add_scalar(\"Loss/val\", loss_val, epoch)\n",
    "        \n",
    "    # Printing\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}], TLoss: { total_loss.item() :.7f}, MSELoss: { loss.item() :.7f}, Val_loss: {loss_val.item() :.5f}\")\n",
    "        \n",
    "\n",
    "if use_wandb: wandb.finish()\n",
    "if use_tb:\n",
    "    tb_writer.flush()\n",
    "    tb_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save and load the entire model.\n",
    "torch.save(model, dir_models/f'{param_save}_model.ckpt')\n",
    "# # model = torch.load('model.ckpt')\n",
    "\n",
    "# # Save and load only the model parameters (recommended).\n",
    "torch.save(model.state_dict(), dir_models/f'{param_save}_params.ckpt')\n",
    "# # resnet.load_state_dict(torch.load('params.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0988)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(y_val,torch.zeros_like(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      2\u001b[0m S\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 3\u001b[0m features \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mrange\u001b[39m(n_output),S)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      5\u001b[0m     fig,axes \u001b[38;5;241m=\u001b[39mplt\u001b[38;5;241m.\u001b[39msubplots(S)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_output' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "S=10\n",
    "features = random.sample(range(n_output),S)\n",
    "with torch.no_grad():\n",
    "    fig,axes =plt.subplots(S)\n",
    "    for i, (ax,index) in enumerate(zip(axes,features)):\n",
    "        ax.plot(y_train[:200,index],\"b\")\n",
    "        ax.plot(outputs[:200,index],\"r\")\n",
    "    fig.suptitle(\"Training\")\n",
    "    \n",
    "with torch.no_grad():\n",
    "    \n",
    "    fig,axes =plt.subplots(S)\n",
    "    for i, (ax,index) in enumerate(zip(axes,features)):\n",
    "        ax.plot(y_val[:200,index],\"b\")\n",
    "        ax.plot(outputs_val[:200,index],\"r\")\n",
    "    fig.suptitle(\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3813, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(outputs,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6905, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(outputs[:100],y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # plt.hist(y_renorm(y_train).sum(dim=1),bins=20,color=\"b\",alpha=0.5)\n",
    "    plt.hist(y_renorm(outputs).sum(dim=1),bins=20,color=\"r\",alpha=0.5)\n",
    "    plt.hist(y_renorm(outputs_val).sum(dim=1),bins=20,color=\"g\",alpha=0.5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "snapshot\n",
       "2012-01-01 00:00:00    0.000264\n",
       "2012-01-01 03:00:00    0.000259\n",
       "2012-01-01 06:00:00    0.000268\n",
       "2012-01-01 09:00:00    0.000268\n",
       "2012-01-01 12:00:00    0.000294\n",
       "                         ...   \n",
       "2012-12-31 09:00:00    0.000721\n",
       "2012-12-31 12:00:00    0.000712\n",
       "2012-12-31 15:00:00    0.000730\n",
       "2012-12-31 18:00:00    0.000825\n",
       "2012-12-31 21:00:00    0.000814\n",
       "Length: 2928, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output_val.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (132240847.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [16], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    y_train.\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('phd_ph5x-02_emulator_deepOPF_pypsa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b392f132ec8d9f75c9dc3d7bf67657658141c2cfa78fd6226f20eb67aeb16219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
