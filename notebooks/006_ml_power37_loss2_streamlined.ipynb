{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skeleton pytorch with PyPSA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "# from torchvision.utils import make_grid\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12,4)\n",
    "# matplotlib.rcParams['figure.facecolor'] = '#ffffff'\n",
    "\n",
    "# Logging ML\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa525b822b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict(    \n",
    "    learning_rate =1e-3,\n",
    "    # batch_size =128,\n",
    "    epochs = int(5e3),\n",
    "    model=\"nn5\",\n",
    "    layers = [256,512,512,512,256,128,64],\n",
    "    training_loss = \"MSE+tP\",\n",
    "    alpha= 1e-6,\n",
    "    # scheduler = \"one-cycle-lr\",\n",
    "    years_list=[\"2010\",\"2011\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\"],\n",
    "    years_list_val=[\"2012\"],\n",
    "    nodes=\"37\"\n",
    ")\n",
    "assert set(config[\"years_list\"]).isdisjoint(config[\"years_list_val\"]), \"Validation set contained in training data!\"\n",
    "\n",
    "config['n_years'] = len(config[\"years_list\"])\n",
    "config['n_years_val'] = len(config[\"years_list_val\"])\n",
    "\n",
    "use_tb = False\n",
    "use_wandb= True\n",
    "manual_logging = False\n",
    "check_data_with_plots = False\n",
    "\n",
    "project_name = f\"phd-ph5-01-power_prediction_{config['nodes']}\"\n",
    "\n",
    "random_seed = 746435\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otherwise all files present\n"
     ]
    }
   ],
   "source": [
    "dir_root = Path(\"../\") # go to root of git repo\n",
    "dir_data = dir_root / \"data\"\n",
    "dir_data_ml= dir_data /\"ml\"\n",
    "dir_models = dir_root / \"models\"\n",
    "dir_runs = dir_root/\"runs\"\n",
    "dir_runs_tb = dir_runs /\"tb\"\n",
    "dir_runs_wandb = dir_root / \"wandb\"\n",
    "param_save = \"003_3_model_nn5\"\n",
    "\n",
    "network_name = f\"elec_s_{config['nodes']}_ec_lcopt_Co2L-3H\"\n",
    "\n",
    "dir_training_set = [dir_data_ml / y / \"3M\" for y in config[\"years_list\"]]\n",
    "filenames_inputs_tr = [d / f\"{network_name}_inputs.P\" for d in dir_training_set]\n",
    "filenames_outputs_tr = [d / f\"{network_name}_outputs_p.P\" for d in dir_training_set]\n",
    "\n",
    "dir_val_set = [ dir_data_ml / y/ \"3M\" for y in  config[\"years_list_val\"]]\n",
    "filenames_inputs_val = [d / f\"{network_name}_inputs.P\" for d in dir_val_set]\n",
    "filenames_outputs_val = [d / f\"{network_name}_outputs_p.P\" for d in dir_val_set]\n",
    "\n",
    "for fn in [*filenames_inputs_tr, *filenames_outputs_tr,\n",
    "           *filenames_inputs_val,*filenames_outputs_val]:\n",
    "    if not fn.exists():\n",
    "        print(f\"{fn}: Missing\")\n",
    "print(\"Otherwise all files present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 37, 23368, 2928)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_all_dfs(filenames):\n",
    "    return pd.concat([pd.read_pickle(f) for f in filenames])\n",
    "\n",
    "df_input_tr = read_all_dfs(filenames_inputs_tr)\n",
    "df_output_tr = read_all_dfs(filenames_outputs_tr)\n",
    "df_input_val = read_all_dfs(filenames_inputs_val)\n",
    "df_output_val = read_all_dfs(filenames_outputs_val)\n",
    "\n",
    "\n",
    "assert (df_input_val.columns==df_input_tr.columns).all(), \"Mismatch in input columns\"\n",
    "assert (df_output_val.columns==df_output_tr.columns).all(), \"Mismatch in output columns\"\n",
    "input_features = df_input_val.columns\n",
    "output_features = df_output_val.columns\n",
    "\n",
    "x_train = torch.from_numpy(df_input_tr.values.astype(\"float32\"))\n",
    "y_train = torch.from_numpy(df_output_tr.values.astype(\"float32\"))\n",
    "x_val = torch.from_numpy(df_input_val.values.astype(\"float32\"))\n",
    "y_val = torch.from_numpy(df_output_val.values.astype(\"float32\"))\n",
    "\n",
    "n_input = x_train.shape[1]\n",
    "n_output = y_train.shape[1]\n",
    "n_samples_tr = x_train.shape[0]\n",
    "n_samples_val = x_val.shape[0]\n",
    "\n",
    "# Normalization defined by training data\n",
    "x_mean = x_train.mean(dim = 0)\n",
    "x_std =x_train.std(dim = 0)\n",
    "y_mean = torch.zeros(n_output)  # centered already\n",
    "y_std = y_train.std(dim = 0)\n",
    "\n",
    "def x_norm(x): return (x-x_mean)/x_std\n",
    "def y_norm(y): return (y-y_mean)/y_std\n",
    "def x_renorm(x): return x*x_std+x_mean\n",
    "def y_renorm(y): return y*y_std+y_mean\n",
    "\n",
    "\n",
    "x_train =  x_norm(x_train)\n",
    "y_train = y_norm(y_train)\n",
    "\n",
    "x_val = x_norm(x_val)\n",
    "y_val = y_norm(y_val)\n",
    "y_renorm(y_train).sum(dim=1)\n",
    "assert not(((x_val[0:100]-x_train[0:100])<1e-5).all()), \"Training data identical to validation data\"\n",
    "\n",
    "\n",
    "(n_input,n_output,n_samples_tr,n_samples_val)\n",
    "# train_loader = load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_data_with_plots:\n",
    "    n_min=min(n_samples_val,n_samples_tr)\n",
    "    _=plt.plot(x_train[:n_min], \"r\", alpha = 0.1) # [:,38:192]\n",
    "    _=plt.plot(y_train[:n_min], \"b\", alpha = 0.1) # [:,38:192]\n",
    "    _=plt.plot(x_val[:n_min], \"m\", alpha = 0.1) # [:,38:192]\n",
    "    _=plt.plot(y_val[:n_min], \"c\", alpha = 0.1) # [:,38:192]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Full model\n",
    "class PowerModel(nn.Module):\n",
    "    \"\"\"Feedfoward neural network (0 layers)\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(PowerModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        current_dim = input_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "        for hdim in hidden_dim:\n",
    "            self.layers.append(nn.Linear(current_dim, hdim))\n",
    "            current_dim = hdim\n",
    "        self.layers.append(nn.Linear(current_dim, output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        out = self.layers[-1](x)\n",
    "        return out    \n",
    "        \n",
    "(\n",
    "    # def training_step(self, batch):\n",
    "    #     inputs, targets = batch \n",
    "    #     out = self(inputs)                  # Generate predictions\n",
    "    #     loss = F.cross_entropy(out, targets) # Calculate loss\n",
    "    #     return loss\n",
    "    \n",
    "    # def validation_step(self, batch):\n",
    "    #     inputs, targets = batch \n",
    "    #     out = self(inputs)                    # Generate predictions\n",
    "    #     loss = F.cross_entropy(out, targets)   # Calculate loss\n",
    "    #     # acc = accuracy(out, labels)           # Calculate accuracy\n",
    "    #     return loss\n",
    "\n",
    "        \n",
    "    # def validation_epoch_end(self, outputs):\n",
    "    #     batch_losses = outputs\n",
    "    #     epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "    #     # batch_accs = [x['val_acc'] for x in outputs]\n",
    "    #     # epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "    #     return  epoch_loss.item()\n",
    "\n",
    "    # def epoch_end(self,epoch,result):\n",
    "    #     print(f\"Epoch [{epoch}], val_loss: { result['val_loss'] :.4f}\") #, val_acc: {result['val_acc']:.4f}\")\n",
    ")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NN regression model\n",
    "if config[\"model\"]==\"linear\":\n",
    "    model = nn.Linear(n_input,n_output)\n",
    "elif config[\"model\"].startswith(\"nn\"):\n",
    "    model =  PowerModel(n_input,n_output,hidden_dim=config[\"layers\"])\n",
    "    \n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()  \n",
    "def mean_total_power(y): return (y_renorm(y).sum(dim=1)**2).mean(dim=0) #squared\n",
    "# criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "if \"scheduler\" in config:\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,config[\"learning_rate\"],config[\"epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33melkir\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../runs/wandb/run-20221027_124129-77i0ts68</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/elkir/phd-ph5-01-power_prediction_37/runs/77i0ts68\" target=\"_blank\">still-firebrand-3</a></strong> to <a href=\"https://wandb.ai/elkir/phd-ph5-01-power_prediction_37\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiation_MSEloss: 1.8848963975906372\n",
      "Epoch [50], TLoss: 1.8634948, MSELoss: 1.8629802, Val_loss: 1.80454\n",
      "Epoch [100], TLoss: 1.8411549, MSELoss: 1.8411279, Val_loss: 1.78607\n",
      "Epoch [150], TLoss: 1.7986364, MSELoss: 1.7979370, Val_loss: 1.74738\n",
      "Epoch [200], TLoss: 1.4059558, MSELoss: 1.3146722, Val_loss: 1.29357\n",
      "Epoch [250], TLoss: 1.1795915, MSELoss: 1.1242729, Val_loss: 1.07023\n",
      "Epoch [400], TLoss: 1.0617387, MSELoss: 1.0304623, Val_loss: 0.96722\n",
      "Epoch [600], TLoss: 0.9613881, MSELoss: 0.9612660, Val_loss: 0.91289\n",
      "Epoch [800], TLoss: 0.9532174, MSELoss: 0.8984520, Val_loss: 0.87128\n",
      "Epoch [1000], TLoss: 0.8543890, MSELoss: 0.8411720, Val_loss: 0.82352\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "if manual_logging: losses=[]\n",
    "if use_tb: tb_writer= SummaryWriter(log_dir=dir_runs_tb)\n",
    "if use_wandb:  \n",
    "    run = wandb.init(\n",
    "    project=project_name,\n",
    "    dir=dir_runs,config=config)\n",
    "\n",
    "print(f\"Instantiation_MSEloss: {criterion(y_train,model(x_train))}\")\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss_mtP = config['alpha']*mean_total_power(outputs)\n",
    "    \n",
    "    outputs_val = model(x_val)\n",
    "    loss_val = criterion(outputs_val, y_val)\n",
    "    loss_val_mtP = config['alpha']*mean_total_power(outputs_val)\n",
    "    diff_loss= loss_val-loss\n",
    "    \n",
    "    total_loss = loss + loss_mtP\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    # model.epoch_end(epoch,loss)\n",
    "    if \"scheduler\" in config: scheduler.step()\n",
    "    \n",
    "    # Logging\n",
    "    if manual_logging: losses.append(loss.item())\n",
    "    if use_wandb: wandb.log(dict(\n",
    "        total_loss=total_loss,\n",
    "        loss=loss,\n",
    "        loss_mtP=loss_mtP,\n",
    "        loss_val=loss_val,\n",
    "        loss_val_mtP=loss_val_mtP,\n",
    "        diff_loss=diff_loss,\n",
    "        \n",
    "        lr=optimizer.param_groups[0]['lr']\n",
    "        ))\n",
    "    if use_tb:\n",
    "        tb_writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "        tb_writer.add_scalar(\"Loss/val\", loss_val, epoch)\n",
    "        \n",
    "    # Printing\n",
    "    if ((epoch+1) < 300 and (epoch+1) % 50 == 0) or ((epoch+1) % 200 ==0):\n",
    "        print(f\"Epoch [{epoch+1}], TLoss: { total_loss.item() :.7f}, MSELoss: { loss.item() :.7f}, Val_loss: {loss_val.item() :.5f}\")\n",
    "\n",
    "# # Save and load only the model parameters (recommended).\n",
    "if use_wandb:\n",
    "    param_save=wandb.run.id\n",
    "fpath_model_params = dir_models/f'{param_save}_params.ckpt'\n",
    "torch.save(model.state_dict(), fpath_model_params)\n",
    "if use_wandb:\n",
    "    artifact = wandb.Artifact(name = 'model-weigths',type=\"model\")\n",
    "    artifact.add_file(fpath_model_params, name=\"model_params\")\n",
    "    run.log_artifact(artifact)\n",
    "\n",
    "if use_wandb: wandb.finish()\n",
    "if use_tb:\n",
    "    tb_writer.flush()\n",
    "    tb_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save and load the entire model.\n",
    "torch.save(model, dir_models/f'{param_save}_model.ckpt')\n",
    "# # model = torch.load('model.ckpt')\n",
    "\n",
    "# # Save and load only the model parameters (recommended).\n",
    "torch.save(model.state_dict(), dir_models/f'{param_save}_params.ckpt')\n",
    "# # resnet.load_state_dict(torch.load('params.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(y_val,torch.zeros_like(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "S=8\n",
    "features = random.sample(range(n_output),S)\n",
    "with torch.no_grad():\n",
    "    fig,axes =plt.subplots(S)\n",
    "    for i, (ax,index) in enumerate(zip(axes,features)):\n",
    "        ax.plot(y_train[:200,index],\"b\")\n",
    "        ax.plot(outputs[:200,index],\"r\")\n",
    "    fig.suptitle(\"Training\")\n",
    "    \n",
    "with torch.no_grad():\n",
    "    \n",
    "    fig,axes =plt.subplots(S)\n",
    "    for i, (ax,index) in enumerate(zip(axes,features)):\n",
    "        ax.plot(y_val[:200,index],\"b\")\n",
    "        ax.plot(outputs_val[:200,index],\"r\")\n",
    "    fig.suptitle(\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(outputs,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(outputs[:100],y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # plt.hist(y_renorm(y_train).sum(dim=1),bins=20,color=\"b\",alpha=0.5)\n",
    "    plt.hist(y_renorm(outputs).sum(dim=1),bins=20,color=\"r\",alpha=0.5)\n",
    "    plt.hist(y_renorm(outputs_val).sum(dim=1),bins=20,color=\"g\",alpha=0.5)\n",
    "    # plt.hist(y_renorm(y_train).sum(dim=1),bins=20,color=\"b\",alpha=0.5)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output_val.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('phd_ph5-1_emulator_deepOPF_pypsa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f21fd77aa2a6b5e451eef707f5dbf6be0a0ddbd048179748907f22530ee1b3cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
